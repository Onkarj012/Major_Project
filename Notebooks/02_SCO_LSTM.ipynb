{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "474be30b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/opt/anaconda3/lib/python3.12/site-packages/pyarrow/lib.cpython-312-darwin.so, 0x0002): Library not loaded: @rpath/libutf8proc.2.dylib\n  Referenced from: <07D846C5-FC3D-3AA9-8A72-A10CEDBD3657> /opt/anaconda3/lib/libarrow.1900.0.0.dylib\n  Reason: tried: '/opt/anaconda3/lib/libutf8proc.2.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libutf8proc.2.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libutf8proc.2.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libutf8proc.2.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libutf8proc.2.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libutf8proc.2.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libutf8proc.2.dylib' (no such file), '/opt/anaconda3/bin/../lib/libutf8proc.2.dylib' (no such file), '/opt/anaconda3/bin/../lib/libutf8proc.2.dylib' (no such file), '/usr/local/lib/libutf8proc.2.dylib' (no such file), '/usr/lib/libutf8proc.2.dylib' (no such file, not in dyld cache)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcopy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deepcopy \u001b[38;5;28;01mas\u001b[39;00m dc\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/__init__.py:73\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     __check_build,\n\u001b[1;32m     71\u001b[0m     _distributor_init,\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     76\u001b[0m _submodules \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_missing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_scalar_nan\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/__init__.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_indexing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     _safe_indexing,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     resample,\n\u001b[1;32m     19\u001b[0m     shuffle,\n\u001b[1;32m     20\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_chunking.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _deprecate_force_all_finite\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_array_api.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_extra \u001b[38;5;28;01mas\u001b[39;00m xpx\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternals\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# TODO: complete __all__\u001b[39;00m\n\u001b[1;32m     23\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpx\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# we import xpx here just to re-export it, need this to appease ruff\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/fixes.py:421\u001b[0m\n\u001b[1;32m    419\u001b[0m PYARROW_VERSION_BELOW_17 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\n\u001b[1;32m    423\u001b[0m     pyarrow_version \u001b[38;5;241m=\u001b[39m parse_version(pyarrow\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pyarrow_version \u001b[38;5;241m<\u001b[39m parse_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m17.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyarrow/__init__.py:65\u001b[0m\n\u001b[1;32m     63\u001b[0m _gc_enabled \u001b[38;5;241m=\u001b[39m _gc\u001b[38;5;241m.\u001b[39misenabled()\n\u001b[1;32m     64\u001b[0m _gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_lib\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _gc_enabled:\n\u001b[1;32m     67\u001b[0m     _gc\u001b[38;5;241m.\u001b[39menable()\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/opt/anaconda3/lib/python3.12/site-packages/pyarrow/lib.cpython-312-darwin.so, 0x0002): Library not loaded: @rpath/libutf8proc.2.dylib\n  Referenced from: <07D846C5-FC3D-3AA9-8A72-A10CEDBD3657> /opt/anaconda3/lib/libarrow.1900.0.0.dylib\n  Reason: tried: '/opt/anaconda3/lib/libutf8proc.2.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libutf8proc.2.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libutf8proc.2.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libutf8proc.2.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libutf8proc.2.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libutf8proc.2.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libutf8proc.2.dylib' (no such file), '/opt/anaconda3/bin/../lib/libutf8proc.2.dylib' (no such file), '/opt/anaconda3/bin/../lib/libutf8proc.2.dylib' (no such file), '/usr/local/lib/libutf8proc.2.dylib' (no such file), '/usr/lib/libutf8proc.2.dylib' (no such file, not in dyld cache)"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy as dc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "# ---------------- reproducibility ----------------\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# ---------------- device ----------------\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "data_dir = \"../Data\"  # path where your CSVs are stored\n",
    "ticker = 'HDFCBANK.NS'\n",
    "csv_path = os.path.join(data_dir, f\"{ticker}_features.csv\")\n",
    "\n",
    "# Load CSV\n",
    "stock_data = pd.read_csv(csv_path)\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "stock_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Use Close only (keeps pipeline similar), but could add more features later\n",
    "data = stock_data[['Date', 'Close']].copy()\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# ---------------- prepare dataframe function ----------------\n",
    "def prepare_dataframe_for_lstm(df, n_steps):\n",
    "    df = dc(df)\n",
    "    df.set_index('Date', inplace=True)\n",
    "    for i in range(1, n_steps + 1):\n",
    "        df[f'Close(t-{i})'] = df['Close'].shift(i)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "lookback = 30\n",
    "shifted_df = prepare_dataframe_for_lstm(data, lookback)\n",
    "\n",
    "X = shifted_df.drop('Close', axis=1).values    # shape (N, lookback)\n",
    "y = shifted_df['Close'].values                 # shape (N,)\n",
    "\n",
    "# ---------------- scalers ----------------\n",
    "scaler_X = MinMaxScaler(feature_range=(-1, 1))\n",
    "X = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "y = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# keep your original flip\n",
    "X = dc(np.flip(X, axis=1))\n",
    "\n",
    "split_index = int(len(X) * 0.95)\n",
    "X_train_np, X_test_np = X[:split_index], X[split_index:]\n",
    "y_train_np, y_test_np = y[:split_index], y[split_index:]\n",
    "\n",
    "# torch tensors\n",
    "X_train = torch.tensor(X_train_np.reshape((-1, lookback, 1))).float()\n",
    "X_test = torch.tensor(X_test_np.reshape((-1, lookback, 1))).float()\n",
    "y_train = torch.tensor(y_train_np).float()\n",
    "y_test = torch.tensor(y_test_np).float()\n",
    "\n",
    "# ---------------- dataset / dataloader ----------------\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "train_loader = DataLoader(TimeSeriesDataset(X_train, y_train), batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(TimeSeriesDataset(X_test, y_test), batch_size=16, shuffle=False)\n",
    "\n",
    "# ---------------- model (bigger but still small) ----------------\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# instantiate\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "model = LSTMModel(1, hidden_size, num_layers, dropout).to(device)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# --------------- helper functions ---------------\n",
    "def evaluate_loss_on_loader(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            loss = loss_fn(out, yb)\n",
    "            b = xb.size(0)\n",
    "            total_loss += loss.item() * b\n",
    "            total_samples += b\n",
    "    return total_loss / max(1, total_samples)\n",
    "\n",
    "def get_flat_params_from_module(module):\n",
    "    return torch.cat([p.data.view(-1) for p in module.parameters()]).detach()\n",
    "\n",
    "def set_flat_params_to_module(module, flat_params):\n",
    "    idx = 0\n",
    "    for p in module.parameters():\n",
    "        n = p.numel()\n",
    "        p.data.copy_(flat_params[idx: idx + n].view_as(p).to(p.device))\n",
    "        idx += n\n",
    "\n",
    "# --------------- Adam pretraining with early stopping & scheduler ---------------\n",
    "pretrain_epochs = 80            # increase if you want\n",
    "patience = 10                   # early stopping patience on val loss\n",
    "clip_value = 1.0\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "# Removed `verbose` argument for compatibility with PyTorch builds that don't accept it:\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "best_val = float('inf')\n",
    "best_state = None\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(pretrain_epochs):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    count = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = loss_function(out, yb)\n",
    "        loss.backward()\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        optimizer.step()\n",
    "        running += loss.item() * xb.size(0)\n",
    "        count += xb.size(0)\n",
    "    train_loss = running / max(1, count)\n",
    "    val_loss = evaluate_loss_on_loader(model, test_loader, loss_function, device)\n",
    "\n",
    "    # step the scheduler (ReduceLROnPlateau expects the metric)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # manual LR print for visibility (since we removed verbose)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    if (epoch+1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"[Adam] Epoch {epoch+1}/{pretrain_epochs} train_loss={train_loss:.6f} val_loss={val_loss:.6f} lr={current_lr:.2e}\")\n",
    "\n",
    "    # early stopping\n",
    "    if val_loss < best_val - 1e-6:\n",
    "        best_val = val_loss\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    if no_improve >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1} (no improvement in {patience} epochs)\")\n",
    "        break\n",
    "\n",
    "# restore best state if found\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# Metrics after Adam\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_pred = model(X_train.to(device)).cpu().numpy()\n",
    "    test_pred = model(X_test.to(device)).cpu().numpy()\n",
    "\n",
    "train_pred_inv = scaler_y.inverse_transform(train_pred)\n",
    "test_pred_inv = scaler_y.inverse_transform(test_pred)\n",
    "y_train_inv = scaler_y.inverse_transform(y_train.cpu().numpy())\n",
    "y_test_inv = scaler_y.inverse_transform(y_test.cpu().numpy())\n",
    "\n",
    "mae_adam = mean_absolute_error(y_test_inv.flatten(), test_pred_inv.flatten())\n",
    "rmse_adam = math.sqrt(mean_squared_error(y_test_inv.flatten(), test_pred_inv.flatten()))\n",
    "mape_adam = mean_absolute_percentage_error(y_test_inv.flatten(), test_pred_inv.flatten()) * 100\n",
    "acc_adam = 100 - mape_adam\n",
    "\n",
    "print(\"\\nAfter Adam pretraining (metrics):\")\n",
    "print(f\"MAE: {mae_adam:.4f}\")\n",
    "print(f\"RMSE: {rmse_adam:.4f}\")\n",
    "print(f\"MAPE: {mape_adam:.3f}%\")\n",
    "print(f\"Calculated Accuracy: {acc_adam:.3f}%\")\n",
    "\n",
    "# --------------- SandCat-style fine-tune on final fc layer only ---------------\n",
    "class SandCatFC:\n",
    "    \"\"\"\n",
    "    SCO-style optimizer but only optimizing the small fc layer of the model.\n",
    "    This is much safer and faster than modifying whole-network weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_agents=20, sigma=0.02, device='cpu', verbose=True):\n",
    "        self.n_agents = int(n_agents)\n",
    "        self.sigma = float(sigma)\n",
    "        self.device = device\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def optimize(self, model, train_loader, val_loader, loss_fn, epochs=10):\n",
    "        # module to optimize\n",
    "        module = model.fc\n",
    "        best_pos = get_flat_params_from_module(module).to(self.device).clone()\n",
    "        dim = best_pos.numel()\n",
    "        # small scale based on magnitude\n",
    "        base_scale = (best_pos.abs() + 1.0).to(self.device)\n",
    "        # init population around best_pos\n",
    "        pop = best_pos.unsqueeze(0).repeat(self.n_agents, 1) + \\\n",
    "              (self.sigma * torch.randn(self.n_agents, dim, device=self.device) * base_scale.unsqueeze(0))\n",
    "\n",
    "        best_score = float('inf')\n",
    "        for it in range(epochs):\n",
    "            if self.verbose:\n",
    "                print(f\"[SCO-FC] iter {it+1}/{epochs} evaluating {self.n_agents} agents...\")\n",
    "            scores = []\n",
    "            for i in range(self.n_agents):\n",
    "                cand = pop[i]\n",
    "                set_flat_params_to_module(module, cand)\n",
    "                train_loss = evaluate_loss_on_loader(model, train_loader, loss_fn, self.device)\n",
    "                scores.append(train_loss)\n",
    "            scores = np.array(scores, dtype=float)\n",
    "            argmin = int(np.argmin(scores))\n",
    "            if scores[argmin] < best_score:\n",
    "                best_score = float(scores[argmin])\n",
    "                best_pos = pop[argmin].clone()\n",
    "\n",
    "            # set best found, report train/val\n",
    "            set_flat_params_to_module(module, best_pos)\n",
    "            tr = evaluate_loss_on_loader(model, train_loader, loss_fn, self.device)\n",
    "            va = evaluate_loss_on_loader(model, val_loader, loss_fn, self.device) if val_loader is not None else None\n",
    "            if self.verbose:\n",
    "                if va is not None:\n",
    "                    print(f\"[SCO-FC] iter {it+1} best_train={tr:.6f} best_val={va:.6f}\")\n",
    "                else:\n",
    "                    print(f\"[SCO-FC] iter {it+1} best_train={tr:.6f}\")\n",
    "\n",
    "            # shrink search radius and re-sample\n",
    "            a = 2 * (1 - (it / max(1, epochs)))  # decreases approx 2 -> 0\n",
    "            spread = (self.sigma * (a * 0.5 + 0.01)) * base_scale.unsqueeze(0)\n",
    "            pop = best_pos.unsqueeze(0).repeat(self.n_agents, 1) + torch.randn(self.n_agents, dim, device=self.device) * spread\n",
    "            pop[0] = best_pos.clone()\n",
    "\n",
    "        # finalize\n",
    "        set_flat_params_to_module(module, best_pos)\n",
    "        return best_score\n",
    "\n",
    "# run SCO on fc\n",
    "sco = SandCatFC(n_agents=24, sigma=0.02, device=device, verbose=True)\n",
    "sco_epochs = 12\n",
    "start_time = time.time()\n",
    "best_sco_train_loss = sco.optimize(model, train_loader, test_loader, loss_function, epochs=sco_epochs)\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"SCO fine-tune (fc) finished in {elapsed:.1f}s, best SCO train loss: {best_sco_train_loss:.6f}\")\n",
    "\n",
    "# --------------- Evaluate final metrics ---------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_pred2 = model(X_train.to(device)).cpu().numpy()\n",
    "    test_pred2 = model(X_test.to(device)).cpu().numpy()\n",
    "\n",
    "train_pred2_inv = scaler_y.inverse_transform(train_pred2)\n",
    "test_pred2_inv = scaler_y.inverse_transform(test_pred2)\n",
    "y_train_inv = scaler_y.inverse_transform(y_train.cpu().numpy())\n",
    "y_test_inv = scaler_y.inverse_transform(y_test.cpu().numpy())\n",
    "\n",
    "mae_final = mean_absolute_error(y_test_inv.flatten(), test_pred2_inv.flatten())\n",
    "rmse_final = math.sqrt(mean_squared_error(y_test_inv.flatten(), test_pred2_inv.flatten()))\n",
    "mape_final = mean_absolute_percentage_error(y_test_inv.flatten(), test_pred2_inv.flatten()) * 100\n",
    "acc_final = 100 - mape_final\n",
    "\n",
    "print(\"\\nAfter SCO (fc) fine-tuning (metrics):\")\n",
    "print(f\"MAE: {mae_final:.4f}\")\n",
    "print(f\"RMSE: {rmse_final:.4f}\")\n",
    "print(f\"MAPE: {mape_final:.3f}%\")\n",
    "print(f\"Calculated Accuracy: {acc_final:.3f}%\")\n",
    "\n",
    "# --------------- Plot final comparison ---------------\n",
    "def plot_compare(y_true_inv, before_inv, after_inv, title=\"Test Comparison\"):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(y_true_inv.flatten(), label='Actual')\n",
    "    plt.plot(before_inv.flatten(), label='Pred (after Adam)')\n",
    "    plt.plot(after_inv.flatten(), label='Pred (after SCO-fc)')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_compare(y_test_inv, test_pred_inv, test_pred2_inv, title=\"Test Data: Adam vs Adam+SCO-fc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21751acf-cd03-4013-b648-0ee6d8511ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
